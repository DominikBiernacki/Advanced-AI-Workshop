{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensory"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "a"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kopiowanie z numpy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "b_numpy = np.array([1, 2])\n",
    "\n",
    "b = torch.from_numpy(b_numpy)\n",
    "\n",
    "b"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kopiowanie z innych tensorów"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a_0 = torch.zeros_like(a)\n",
    "\n",
    "print(a_0)\n",
    "\n",
    "a_1 = torch.ones_like(a)\n",
    "\n",
    "print(a_1)\n",
    "\n",
    "a_rand = torch.rand_like(a, dtype=float)\n",
    "\n",
    "print(a_rand)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor o danym kształcie"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a_0 = torch.zeros(a.shape)\n",
    "\n",
    "print(a_0)\n",
    "\n",
    "a_1 = torch.ones(a.shape)\n",
    "\n",
    "print(a_1)\n",
    "\n",
    "a_rand = torch.rand(a.shape)\n",
    "\n",
    "print(a_rand)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Własności tensora"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Tensor:\", a)\n",
    "\n",
    "a.size(), a.shape, a.dtype, a.device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indeksowanie tak jak w numpy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a[0,:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Łączenie tensorów"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "t1 = torch.rand((2, 2))*10\n",
    "t2 = torch.rand((2, 2))*10\n",
    "t3 = torch.rand((2, 2))*10\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "t_cat = torch.cat((t1, t2, t3))\n",
    "\n",
    "print(t_cat)\n",
    "print(t_cat.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "t_cat2 = torch.cat((t1, t2, t3), axis=1)\n",
    "\n",
    "print(t_cat2)\n",
    "print(t_cat2.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "t_stack = torch.stack((t1, t2, t3))\n",
    "\n",
    "print(t_stack)\n",
    "print(t_stack.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch a Numpy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "t = torch.ones((2, 2))\n",
    "\n",
    "n = t.numpy()\n",
    "\n",
    "print(n)\n",
    "\n",
    "t.add_(1)\n",
    "\n",
    "print(n)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n = np.ones((2, 2))\n",
    "\n",
    "t = torch.from_numpy(n)\n",
    "\n",
    "print(t)\n",
    "\n",
    "n += 1\n",
    "\n",
    "print(t)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prosty model w pytorch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = Model()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Trening modelu"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, log_every):\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, labels) in enumerate(dataloader):\n",
    "        pred = model(data)\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % log_every == 0:\n",
    "            loss = loss.item()\n",
    "            print(f\"Loss: {loss}\")\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch: {e+1}\")\n",
    "    train_loop(train_loader, 100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ważne warstwy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.nn.Linear(in_features=10, out_features=100) # Warstwa liniowa - mnożenie przez macierz\n",
    "\n",
    "torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=3) # Warstwa konwolucyjna\n",
    "\n",
    "torch.nn.functional.max_pool2d(input=torch.tensor([[[1,2], [3,4]]]), kernel_size=2)\n",
    "\n",
    "torch.nn.Flatten()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje aktywacyjne"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.nn.Sigmoid()\n",
    "torch.nn.ReLU()\n",
    "torch.nn.LeakyReLU()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Optimizery"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
